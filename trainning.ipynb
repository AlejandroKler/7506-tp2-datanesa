{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "np.random.seed(10)\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import randint as sp_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de los df de features para entrenar y predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [\"18_20\",\"19_21\",\"20_22\",\"21_23\"]\n",
    "features = {}\n",
    "label_auc = {}\n",
    "label_inst = {}\n",
    "label_clas_auc = {}\n",
    "label_clas_inst = {}\n",
    "\n",
    "for window in windows:\n",
    "    features[window] = pd.read_csv(\"windows/{}/features.csv\".format(window), index_col=\"ref_hash\")\n",
    "    label_auc[window] = pd.read_csv(\"windows/{}/labels_auc.csv\".format(window), index_col=\"ref_hash\")\n",
    "    label_inst[window] = pd.read_csv(\"windows/{}/labels_inst.csv\".format(window), index_col=\"ref_hash\")\n",
    "    label_clas_auc[window] = pd.DataFrame({'ref_hash': label_auc[window].index, 'label_auc': (label_auc[window]['label_auc'] == 259200).astype(int)}).set_index(\"ref_hash\")\n",
    "    label_clas_inst[window] = pd.DataFrame({'ref_hash': label_inst[window].index, 'label_inst': (label_inst[window]['label_inst'] == 259200).astype(int)}).set_index(\"ref_hash\")\n",
    "    \n",
    "features_to_predict = pd.read_csv(\"windows/24_26/features.csv\", index_col=\"ref_hash\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df, feature_list):\n",
    "    return df.reindex(columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_class_auc = [\n",
    "    'appearances_in_auctions', \n",
    "    'user_appeared_last_day', \n",
    "    'time_to_reappear',\n",
    "    'amount_of_clicks', \n",
    "    'has_installed', \n",
    "    #'user_clicked_last_day',\n",
    "    'user_installed_last_day', \n",
    "    'amount_of_installs', \n",
    "    #'cant_max_day',\n",
    "    #'cant_min_day', \n",
    "    #'mean_time_to_click', \n",
    "    #'max_time_click',\n",
    "    #'min_time_click', \n",
    "    'mean_auctions_per_day', \n",
    "    'mean_events_per_day',\n",
    "    #'mean_clicks_per_day', \n",
    "    'amount_auctions_in_last_hour',\n",
    "    'amount_auctions_in_last_2_hours', \n",
    "    'amount_auctions_in_last_5_hours',\n",
    "    'amount_auctions_in_last_12_hours', \n",
    "    'amount_auctions_in_last_24_hours',\n",
    "    'amount_events_in_last_hour', \n",
    "    'amount_events_in_last_2_hours',\n",
    "    'amount_auctions_in_first_hour', \n",
    "    'amount_auctions_in_first_3_hours',\n",
    "    'amount_auctions_in_first_5_hours', \n",
    "    'amount_auctions_in_first_12_hours',\n",
    "    'amount_events_in_first_hour', \n",
    "    'amount_events_in_first_5_hours',\n",
    "    'amount_events_in_first_12_hours', \n",
    "    #'amount_clicks_in_last_2_hours',\n",
    "    #'amount_clicks_in_last_4_hours', \n",
    "    'device_os', \n",
    "    #'std_time_to_click',\n",
    "    'std_time_auctions', \n",
    "    #'max_time_install', \n",
    "    #'min_time_install',\n",
    "    #'mean_time_install', \n",
    "    #'std_time_install', \n",
    "    'max_time_events',\n",
    "    'min_time_events', \n",
    "    'mean_time_events', \n",
    "    'std_time_events',\n",
    "    'installs_per_events', \n",
    "    #'installs_per_clicks', \n",
    "    'events_x_app_210',\n",
    "    'events_x_app_122', \n",
    "    'events_x_app_65', \n",
    "    'events_x_app_121',\n",
    "    'events_x_app_26', \n",
    "    'most_installed_apps_used', \n",
    "    'cant_apps_used',\n",
    "    #'cant_events_atributed', \n",
    "    #'has_events_atributed',\n",
    "    'has_events_ids_with_installs', \n",
    "    #'has_events_ids_without_installs',\n",
    "    'cant_events_0_4', \n",
    "    'cant_events_4_8', \n",
    "    'cant_events_8_12',\n",
    "    'cant_events_12_16', \n",
    "    'cant_events_16_20', \n",
    "    'cant_events_20_24',\n",
    "    'cant_auctions_0_4', \n",
    "    'cant_auctions_4_8', \n",
    "    'cant_auctions_8_12',\n",
    "    'cant_auctions_12_16', \n",
    "    'cant_auctions_16_20', \n",
    "    'cant_auctions_20_24',\n",
    "    #'implicit', \n",
    "    #'latitude', \n",
    "    #'longitude', \n",
    "    #'clicked_in_last_5_minutes',\n",
    "    #'clicked_with_wifi_in_last_3_hours', \n",
    "    #'hour_install', \n",
    "    'hour_events',\n",
    "    #'hour_clicks', \n",
    "    'hour_auctions',\n",
    "    'amount_auctions_in_last_half_hour',\n",
    "    'amount_auctions_in_last_15_minutes',\n",
    "    #\"mean_appearances_events\",\n",
    "    #\"std_appearances_events\",\n",
    "    \"std_appearances_auctions\",\n",
    "    'cant_devices',\n",
    "    \"mean_appearances_auctions\",\n",
    "    \"mean_of_first_auction\",\n",
    "    \"mean_of_first_event\"\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_class_inst = [\n",
    "    'appearances_in_auctions', \n",
    "    'user_appeared_last_day', \n",
    "    'time_to_reappear',\n",
    "    #'amount_of_clicks', \n",
    "    'has_installed', \n",
    "    'user_clicked_last_day',\n",
    "    'user_installed_last_day', \n",
    "    'amount_of_installs', \n",
    "    #'cant_max_day',\n",
    "    #'cant_min_day', \n",
    "    #'mean_time_to_click', \n",
    "    #'max_time_click',\n",
    "    #'min_time_click', \n",
    "    'mean_auctions_per_day', \n",
    "    'mean_events_per_day',\n",
    "    #'mean_clicks_per_day', \n",
    "    'amount_auctions_in_last_hour',\n",
    "    'amount_auctions_in_last_2_hours', \n",
    "    'amount_auctions_in_last_5_hours',\n",
    "    'amount_auctions_in_last_12_hours', \n",
    "    'amount_auctions_in_last_24_hours',\n",
    "    'amount_events_in_last_hour', \n",
    "    'amount_events_in_last_2_hours',\n",
    "    'amount_auctions_in_first_hour', \n",
    "    'amount_auctions_in_first_3_hours',\n",
    "    'amount_auctions_in_first_5_hours', \n",
    "    'amount_auctions_in_first_12_hours',\n",
    "    'amount_events_in_first_hour', \n",
    "    'amount_events_in_first_5_hours',\n",
    "    'amount_events_in_first_12_hours', \n",
    "    #'amount_clicks_in_last_2_hours',\n",
    "    #'amount_clicks_in_last_4_hours', \n",
    "    'device_os', \n",
    "    #'std_time_to_click',\n",
    "    'std_time_auctions', \n",
    "    #'max_time_install', \n",
    "    #'min_time_install',\n",
    "    #'mean_time_install', \n",
    "    #'std_time_install', \n",
    "    'max_time_events',\n",
    "    'min_time_events', \n",
    "    'mean_time_events', \n",
    "    'std_time_events',\n",
    "    'installs_per_events', \n",
    "    #'installs_per_clicks', \n",
    "    'events_x_app_210',\n",
    "    'events_x_app_122', \n",
    "    'events_x_app_65', \n",
    "    'events_x_app_121',\n",
    "    'events_x_app_26', \n",
    "    'most_installed_apps_used', \n",
    "    'cant_apps_used',\n",
    "    #'cant_events_atributed', \n",
    "    #'has_events_atributed',\n",
    "    'has_events_ids_with_installs', \n",
    "    #'has_events_ids_without_installs',\n",
    "    'cant_events_0_4', \n",
    "    'cant_events_4_8', \n",
    "    'cant_events_8_12',\n",
    "    'cant_events_12_16', \n",
    "    'cant_events_16_20', \n",
    "    'cant_events_20_24',\n",
    "    'cant_auctions_0_4', \n",
    "    'cant_auctions_4_8', \n",
    "    'cant_auctions_8_12',\n",
    "    'cant_auctions_12_16', \n",
    "    'cant_auctions_16_20', \n",
    "    'cant_auctions_20_24',\n",
    "    #'implicit', \n",
    "    #'latitude', \n",
    "    #'longitude', \n",
    "    #'clicked_in_last_5_minutes',\n",
    "    #'clicked_with_wifi_in_last_3_hours', \n",
    "    #'hour_install', \n",
    "    'hour_events',\n",
    "    #'hour_clicks', \n",
    "    'hour_auctions',\n",
    "    #'cant_installs_0_4',\n",
    "    #'cant_installs_4_8', \n",
    "    #'cant_installs_8_12',\n",
    "    #'cant_installs_12_16',\n",
    "    #'cant_installs_16_20',\n",
    "    #'cant_installs_20_24',\n",
    "    #\"mean_appearances_clicks\",\n",
    "    #\"mean_appearances_installs\"\n",
    "    'amount_auctions_in_last_half_hour',\n",
    "    'amount_auctions_in_last_15_minutes',\n",
    "    'cant_devices', \n",
    "    \"mean_appearances_auctions\",\n",
    "   #\"mean_appearances_events\",\n",
    "   #\"std_appearances_events\",\n",
    "    \"std_appearances_auctions\",\n",
    "    \"mean_of_first_auction\",\n",
    "    \"mean_of_first_event\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones útiles para el entrenamento y el balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_to_train(windows, features, labels, feature_list):\n",
    "    df_list = []\n",
    "    for window in windows:\n",
    "        df = select_features(features[window], feature_list).join(labels[window], how=\"inner\")\n",
    "        df_list.append(df)\n",
    "    df_full = pd.concat(df_list)\n",
    "    df_full.reset_index(inplace=True, drop=True)\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor = 1 100% balanced\n",
    "# factor = 0 not balanced\n",
    "# label_value label que mas aparece y se debe balancear\n",
    "def balance(df_full, label_name, factor, label_value = 1):\n",
    "    cant_values = df_full[label_name].value_counts()[label_value]\n",
    "    cant_no_values = len(df_full[label_name]) - cant_values\n",
    "    a_borrar = int((cant_values-cant_no_values)*factor)\n",
    "    index_to_drop = np.random.choice(df_full.loc[df_full[label_name] == label_value].index,a_borrar, replace=False)\n",
    "    return df_full.drop(index=index_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(df_full, label_name, test_size=0.3):\n",
    "    y = df_full[label_name]\n",
    "    X_data = df_full.drop(label_name, axis=1)\n",
    "    return train_test_split(X_data, y, test_size=test_size, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importancia de los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ***Importancia de los features de auctions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_importance = get_df_to_train(windows, features, label_auc, features_list_class_auc)\n",
    "df_auctions_importance = df_auctions_importance.sample(10000)\n",
    "\n",
    "\n",
    "df_auctions_features = df_auctions_importance.drop('label_auc', axis=1)\n",
    "df_auctions_labels = df_auctions_importance['label_auc']\n",
    "\n",
    "X = df_auctions_features.fillna(0)\n",
    "Y = df_auctions_labels\n",
    "names = X.columns.tolist()\n",
    "rf_auc = RandomForestRegressor(n_estimators=100)\n",
    "rf_auc.fit(X, Y)\n",
    "#print(\"Features sorted by their score:\")\n",
    "#print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_auctions = sorted(zip(map(lambda x: round(x, 4), rf_auc.feature_importances_), names))\n",
    "feature_auctions_plot = pd.DataFrame(feature_importance_auctions).set_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot = feature_auctions_plot.plot(kind=\"barh\", figsize=(14,18))\n",
    "plot.set_title(\"Importance of auctions features\", fontsize=25)\n",
    "plot.set_ylabel(\"Feature\", fontsize=20)\n",
    "plot.set_xlabel(\"Importance\", fontsize=20);\n",
    "plot.legend([\"Importance\"], loc=4)\n",
    "plot.tick_params(labelsize=13, which='major')\n",
    "plt.savefig(\"importance_auctions.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ***Importancia de los features de installs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_installs_importance = get_df_to_train(windows, features, label_inst, features_list_class_inst)\n",
    "df_installs_importance = df_installs_importance.sample(10000)\n",
    "\n",
    "\n",
    "df_installs_features = df_installs_importance.drop('label_inst', axis=1)\n",
    "df_installs_labels = df_installs_importance['label_inst']\n",
    "\n",
    "X = df_installs_features.fillna(0)\n",
    "Y = df_installs_labels\n",
    "names = X.columns.tolist()\n",
    "rf_inst = RandomForestRegressor(n_estimators=100)\n",
    "rf_inst.fit(X, Y)\n",
    "#print(\"Features sorted by their score:\")\n",
    "#print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_inst = sorted(zip(map(lambda x: round(x, 4), rf_inst.feature_importances_), names))\n",
    "feature_inst_plot = pd.DataFrame(feature_importance_inst).set_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot = feature_inst_plot.plot(kind=\"barh\", figsize=(14,18))\n",
    "plot.set_title(\"Importance of installs features\", fontsize=25)\n",
    "plot.set_ylabel(\"Feature\", fontsize=20)\n",
    "plot.set_xlabel(\"Importance\", fontsize=20);\n",
    "plot.legend([\"Importance\"], loc=4)\n",
    "plot.tick_params(labelsize=13, which='major')\n",
    "plt.savefig(\"importance_installs.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Modelos***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes neuronales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ***Modelo de redes neuronales de predicción de auctions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_auctions = get_df_to_train(windows, features, label_auc, features_list_class_auc)\n",
    "df_full_auctions = df_full_auctions.fillna(0)\n",
    "\n",
    "# Filtro los tiempos máximos.\n",
    "df_full_auctions = df_full_auctions[df_full_auctions[\"label_auc\"] < 259200]\n",
    "\n",
    "# Separo train y test.\n",
    "X_train_auc, X_test_auc, y_train_auc, y_test_auc = get_train_test_split(df_full_auctions, \"label_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el modelo\n",
    "model_auc_neural = Sequential()\n",
    "\n",
    "# Capa de entrada\n",
    "model_auc_neural.add(Dense(10, input_dim=X_train_auc.shape[1], activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "model_auc_neural.add(Dropout(0.2))\n",
    "\n",
    "# Capa con 40 neuronas\n",
    "model_auc_neural.add(Dense(20, activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "#model_auc_neural.add(Dropout(0.2))\n",
    "\n",
    "# Capa con 60 neuronas\n",
    "model_auc_neural.add(Dense(60, activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "#model_auc_neural.add(Dropout(0.2))\n",
    "\n",
    "# Capa con 60 neuronas\n",
    "model_auc_neural.add(Dense(80, activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "#model_auc_neural.add(Dropout(0.2))\n",
    "\n",
    "# Salida del modelo\n",
    "model_auc_neural.add(Dense(1))\n",
    "\n",
    "# Compilo el modelo\n",
    "model_auc_neural.compile(optimizer ='adam', loss = 'mean_squared_error', metrics =[metrics.mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veo el modelo\n",
    "model_auc_neural.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno el modelo\n",
    "model_auc_neural.fit(X_train_auc, y_train_auc, validation_data=(X_test_auc,y_test_auc), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá pones el valor que da el \"mean_squared_error\" del modelo\n",
    "MSE = 4360503664.0435\n",
    "\n",
    "print(f\"RMSE: {MSE**(1/2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ***Modelo de redes neuronales de predicción de installs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_inst = get_df_to_train(windows, features, label_inst, features_list_class_inst)\n",
    "df_full_inst = df_full_inst.fillna(0)\n",
    "\n",
    "# Saco los tiempos máximos\n",
    "\n",
    "df_full_inst = df_full_inst[df_full_inst[\"label_inst\"] < 259200]\n",
    "#df_full_inst = balance(df_full_inst, \"label_inst\", 1, 259200)\n",
    "\n",
    "\n",
    "# Separo en train y test\n",
    "X_train_inst, X_test_inst, y_train_inst, y_test_inst = get_train_test_split(df_full_inst, \"label_inst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el modelo\n",
    "model_inst_neural = Sequential()\n",
    "\n",
    "# Capa de entrada\n",
    "model_inst_neural.add(Dense(10, input_dim=X_train_inst.shape[1], activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "model_inst_neural.add(Dropout(0.2))\n",
    "\n",
    "# Capa con 30 neuronas\n",
    "model_inst_neural.add(Dense(40, activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "model_inst_neural.add(Dropout(0.2))\n",
    "\n",
    "# Capa con 50 neuronas\n",
    "model_inst_neural.add(Dense(60, activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "\n",
    "# Capa con 20 neuronas\n",
    "model_inst_neural.add(Dense(30, activation='relu'))\n",
    "# Dropout para controlar el overfit\n",
    "model_inst_neural.add(Dropout(0.2))\n",
    "\n",
    "# Salida del modelo\n",
    "model_inst_neural.add(Dense(1))\n",
    "\n",
    "# Compilo el modelo\n",
    "model_inst_neural.compile(optimizer ='adam', loss = 'mean_squared_error', metrics =[metrics.mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veo el modelo\n",
    "model_inst_neural.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno el modelo\n",
    "model_inst_neural.fit(X_train_inst, y_train_inst, validation_data=(X_test_inst,y_test_inst), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá pones el valor que da el \"mean_squared_error\" del modelo\n",
    "MSE = 6272147912.2780\n",
    "\n",
    "print(f\"RMSE: {MSE**(1/2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ***Modelo de XGBoost de predicción de auctions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = get_df_to_train(windows, features, label_auc, features_list_class_auc)\n",
    "\n",
    "df_full = df_full.sample(int(len(df_full)))\n",
    "\n",
    "#df_full = balance(df_full, \"label_auc\", 0, 259200)\n",
    "\n",
    "df_full = df_full.loc[df_full[\"label_auc\"] != 259200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_split(df_full, \"label_auc\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc = xgb.XGBRegressor(   \n",
    "    gamma=0.1, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3,  \n",
    "    n_estimators=500,\n",
    "    n_jobs=2,  \n",
    "    objective='reg:linear',   \n",
    "    silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_auc.fit(X_train, y_train, eval_metric='rmse')\n",
    "#cv = xgb.cv(params, xg_train, 5000, nfold=n_folds, early_stopping_rounds=early_stopping, verbose_eval=1)\n",
    "prediction = model_auc.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc.save_model('xgboost_auc.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### ***Mejor modelo de auctions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE: 63345.641573\n",
    "CPU times: user 25min 48s, sys: 8.65 s, total: 25min 57s\n",
    "Wall time: 26min 8s\n",
    "    \n",
    "model_auc = xgb.XGBRegressor(\n",
    "    base_score=0.5, \n",
    "    booster='gbtree', \n",
    "    colsample_bylevel=1,\n",
    "    colsample_bytree=1, \n",
    "    gamma=0.1, \n",
    "    learning_rate=0.1, \n",
    "    max_delta_step=0,\n",
    "    max_depth=4, \n",
    "    min_child_weight=1, \n",
    "    missing=None, \n",
    "    n_estimators=600,\n",
    "    n_jobs=1, \n",
    "    nthread=None, \n",
    "    objective='reg:linear', \n",
    "    random_state=0,\n",
    "    reg_alpha=0, \n",
    "    reg_lambda=1, \n",
    "    scale_pos_weight=1, \n",
    "    seed=None,\n",
    "    silent=True, \n",
    "    subsample=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_importance(model_auc, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance, plot_tree\n",
    "_ = plot_importance(model_auc, height=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ***Modelo de XGBoost de predicción de installs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_installs = get_df_to_train(windows, features, label_inst, features_list_class_inst)\n",
    "\n",
    "df_full_installs = df_full_installs.sample(int(len(df_full_installs)))\n",
    "\n",
    "#df_full_installs = balance(df_full_installs, \"label_inst\", 1, 259200)\n",
    "df_full_installs = df_full_installs[df_full_installs[\"label_inst\"] != 259200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_split(df_full_installs, \"label_inst\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inst = xgb.XGBRegressor(\n",
    "    learning_rate = 0.1,\n",
    "    n_estimators=700,\n",
    "    max_dept=5,\n",
    "    gamma=0.1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_inst.fit(X_train, y_train, eval_metric='rmse', verbose=True)\n",
    "\n",
    "prediction = model_inst.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inst.save_model('xgboost_inst.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance, plot_tree\n",
    "_ = plot_importance(model_inst, height=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### ***Mejor modelo de installs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
    "RMSE: 76768.119164\n",
    "CPU times: user 5min 39s, sys: 2 s, total: 5min 41s\n",
    "Wall time: 5min 44s\n",
    "\n",
    "model_inst = xgb.XGBRegressor(\n",
    "    learning_rate = 0.1,\n",
    "    n_estimators=700,\n",
    "    max_dept=5,\n",
    "    gamma=0.1,\n",
    "    n_jobs=-1\n",
    ")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LightGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM para auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_auctions = get_df_to_train(windows, features, label_auc, features_list_class_auc)\n",
    "df_full_auctions = df_full_auctions[df_full_auctions[\"label_auc\"] != 259200]\n",
    "#df_full_auctions = df_full_auctions.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_auc, X_test_auc, y_train_auc, y_test_auc = get_train_test_split(df_full_auctions, \"label_auc\", 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train_auc, y_train_auc)\n",
    "lgb_eval = lgb.Dataset(X_test_auc, y_test_auc, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : 500,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 28,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 326,\n",
    "    'min_child_weight': 10000.0,\n",
    "    'reg_alpha': 50,\n",
    "    'reg_lambda': 0,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm_auc = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "#gbm_auc.save_model('model_light_gbm_auc.txt')\n",
    "\n",
    "pred_auc_gbm = gbm_auc.predict(X_test_auc, num_iteration=gbm_auc.best_iteration)\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test_auc, pred_auc_gbm) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM para installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_installs = get_df_to_train(windows, features, label_inst, features_list_class_inst)\n",
    "df_full_installs = df_full_installs[df_full_installs[\"label_inst\"] != 259200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_inst, X_test_inst, y_train_inst, y_test_inst = get_train_test_split(df_full_installs, \"label_inst\", 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train_inst, y_train_inst)\n",
    "lgb_eval = lgb.Dataset(X_test_inst, y_test_inst, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : 1000,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 185,\n",
    "    'min_child_weight': 100.0,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm_inst = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "#gbm_inst.save_model('model_light_gbm_inst.txt')\n",
    "\n",
    "pred_inst_gbm = gbm_inst.predict(X_test_inst, num_iteration=gbm_inst.best_iteration)\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test_inst, pred_inst_gbm) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo de hiperparámetros para LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Tuneo para el modelo de installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='l2', n_jobs=4, n_estimators = 500)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs.fit(X_train_inst, y_train_inst)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gs.best_score_*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_auc = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='l2', n_jobs=4, n_estimators = 500)\n",
    "gs_auc = RandomizedSearchCV(\n",
    "    estimator=clf_auc, param_distributions=param_test, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Tuneo para el modelo de auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_auctions = df_full_auctions.sample(160000)\n",
    "X_train_auc, X_test_auc, y_train_auc, y_test_auc = get_train_test_split(df_full_auctions, \"label_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs_auc.fit(X_train_auc, y_train_auc)\n",
    "print('Best score reached: {} with params: {} '.format(gs_auc.best_score_, gs_auc.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gs_auc.best_score_*(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_auc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest para auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_auctions = get_df_to_train(windows, features, label_auc, features_list_class_auc)\n",
    "df_full_auctions = df_full_auctions[df_full_auctions[\"label_auc\"] != 259200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_auc, X_test_auc, y_train_auc, y_test_auc = get_train_test_split(df_full_auctions, \"label_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_auc = RandomForestRegressor(\n",
    "                           n_estimators=100, \n",
    "                           min_samples_split = 10, \n",
    "                           random_state=100\n",
    "                         )\n",
    "\n",
    "#random forest no maneja nulos\n",
    "X_train_auc = X_train_auc.fillna(0)\n",
    "X_test_auc = X_test_auc.fillna(0) \n",
    "\n",
    "random_forest_auc.fit(X_train_auc, y_train_auc.values.ravel())\n",
    "\n",
    "params = {'objective': 'reg:squarederror',\n",
    "          'eta': 0.3,\n",
    "          'max_depth': 5,\n",
    "          'min_child_weight': 3,\n",
    "          'silent': 1,\n",
    "          'subsample': 0.7,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'seed': 1}\n",
    "\n",
    "num_trees=250\n",
    "\n",
    "prediction_auc = random_forest_auc.predict(X_test_auc)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_auc, prediction_auc))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest para installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_inst = get_df_to_train(windows, features, label_inst, features_list_class_inst)\n",
    "df_full_inst = df_full_inst[df_full_inst[\"label_inst\"] != 259200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_inst, X_test_inst, y_train_inst, y_test_inst = get_train_test_split(df_full_inst, \"label_inst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_inst = RandomForestRegressor(\n",
    "                           n_estimators=100, \n",
    "                           min_samples_split = 10, \n",
    "                           random_state=100\n",
    "                         )\n",
    "\n",
    "#random forest no maneja nulos\n",
    "X_train_inst = X_train_inst.fillna(0)\n",
    "X_test_inst = X_test_inst.fillna(0) \n",
    "\n",
    "random_forest_inst.fit(X_train_inst, y_train_inst.values.ravel())\n",
    "\n",
    "params = {'objective': 'reg:squarederror',\n",
    "          'eta': 0.3,\n",
    "          'max_depth': 5,\n",
    "          'min_child_weight': 3,\n",
    "          'silent': 1,\n",
    "          'subsample': 0.7,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'seed': 1}\n",
    "\n",
    "num_trees=250\n",
    "\n",
    "prediction_inst = random_forest_inst.predict(X_test_inst)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inst, prediction_inst))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción auctions XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auctions = model_auc.predict(select_features(features_to_predict, features_list_class_auc))\n",
    "df_preds_auctions = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : pred_auctions})\n",
    "df_preds_auctions.to_csv(\"auctions_predictions_xgboost.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción auctions Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auctions_neural = model_auc_neural.predict(select_features(features_to_predict, features_list_class_auc).fillna(0))\n",
    "pred_auctions_neural = np.array(pred_auctions_neural).flatten()\n",
    "df_preds_auctions_neural = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : pred_auctions_neural})\n",
    "df_preds_auctions_neural.to_csv(\"auctions_predictions_neural.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción auctions LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auc_gbm = gbm_auc.predict(select_features(features_to_predict,features_list_class_auc),\\\n",
    "                               num_iteration=gbm_auc.best_iteration)\n",
    "df_preds_auc_gbm = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : pred_auc_gbm})\n",
    "df_preds_auc_gbm.to_csv(\"auctions_predictions_lightgbm.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción auctions Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_auc_rf = random_forest_auc.predict(select_features(features_to_predict,features_list_class_auc).fillna(0))\n",
    "df_preds_auc_rf = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : preds_auc_rf})\n",
    "df_preds_auc_rf.to_csv(\"auctions_predictions_randomForest.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción installs XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_installs = model_inst.predict(select_features(features_to_predict, features_list_class_inst))\n",
    "df_preds_installs = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : pred_installs})\n",
    "df_preds_installs.to_csv(\"installs_predictions_xgboost.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción installs Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_installs_neural = model_inst_neural.predict(select_features(features_to_predict, features_list_class_inst).fillna(0))\n",
    "pred_installs_neural = np.array(pred_installs_neural).flatten()\n",
    "df_preds_installs_neural = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : pred_installs_neural})\n",
    "df_preds_installs_neural.to_csv(\"installs_predictions_neural.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción installs LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inst_gbm = gbm_inst.predict(select_features(features_to_predict,features_list_class_inst),\\\n",
    "                               num_iteration=gbm_inst.best_iteration)\n",
    "df_preds_inst_gbm = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : pred_inst_gbm})\n",
    "df_preds_inst_gbm.to_csv(\"installs_predictions_lightgbm.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción installs Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_inst_rf = random_forest_inst.predict(select_features(features_to_predict,features_list_class_inst).fillna(0))\n",
    "df_preds_inst_rf = pd.DataFrame({'ref_hash' : features_to_predict.index, 'obj' : preds_inst_rf})\n",
    "df_preds_inst_rf.to_csv(\"installs_predictions_randomForest.csv\", index=False) #persistimos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning para XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = get_df_to_train(windows, features, label_auc, features_list_class_auc)\n",
    "df_full = df_full.sample(int(len(df_full)*0.1))\n",
    "df_full = df_full[df_full[\"label_auc\"] != 259200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max_depth and min_child_weight tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring= 'neg_mean_squared_error', n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(df_full[features_list_class_auc],df_full['label_auc'])\n",
    "#gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gsearch1.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Refinamos la búsqueda entre valores acotados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth': [3,4,5],\n",
    " 'min_child_weight': [3,4,5]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBRegressor(learning_rate =0.01, n_estimators=1000, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test2, scoring= 'neg_mean_squared_error', n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(df_full[features_list_class_auc],df_full['label_auc'])\n",
    "#gsearch2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch2.best_params_  #los mejores parámetros son max_depth 4 y min child_weight 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gsearch2.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate and n_estimators tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    "    'n_estimators' : [100,200,500, 1000],\n",
    "    'learning_rate' : [0.1, 0.05, 0.01]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBRegressor(learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test3, scoring= 'neg_mean_squared_error', n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(df_full[features_list_class_auc],df_full['label_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gsearch3.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=4,\n",
    " min_child_weight=3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test4, scoring= 'neg_mean_squared_error', n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(df_full[features_list_class_auc],df_full['label_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gsearch4.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsample and colsample_bytree tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBRegressor( learning_rate = 0.1, n_estimators=200, max_depth=4,\n",
    " min_child_weight=3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test5, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(df_full[features_list_class_auc],df_full['label_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gsearch5.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBRegressor( learning_rate = 0.1, n_estimators=200, max_depth=4,\n",
    " min_child_weight=3, gamma=0, subsample=0.9, colsample_bytree=0.7,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test6, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch6.fit(df_full[features_list_class_auc],df_full['label_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(gsearch6.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df(df, name):\n",
    "    df.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las predicciones tendrán seteadas como índice los ref_hash para no perder la referencia\n",
    "No es necesario filtrar los ref_hash y quedarnos solo con los target en las predicciones que obtenemos ya que de eso\n",
    "se encarga la función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"target_competencia_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_preds_installs = pd.read_csv(\"installs_predictions.csv\")\n",
    "#df_preds_auctions = pd.read_csv(\"auctions_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submit_df(auctions_predictions, installs_predictions, target):\n",
    "    \n",
    "    target = target.set_index('ref_hash')\n",
    "    \n",
    "    auc = auctions_predictions.reset_index()\n",
    "    auc.columns = ['ref_hash','obj']\n",
    "    auc['ref_hash'] = auc['ref_hash'].astype(str) + \"_st\"\n",
    "    auc = auc.set_index('ref_hash')\n",
    "    \n",
    "    ins = installs_predictions.reset_index()\n",
    "    ins.columns = ['ref_hash','obj']\n",
    "    ins['ref_hash'] = ins['ref_hash'].astype(str) + \"_sc\"\n",
    "    ins = ins.set_index('ref_hash')\n",
    "    \n",
    "    frames = [ins,auc]\n",
    "    submit_result = pd.concat(frames).reset_index()\n",
    "    target_list = target.reset_index('ref_hash')['ref_hash'].tolist()\n",
    "    return submit_result.loc[submit_result['ref_hash'].isin(target_list)].sort_values(by = 'ref_hash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_sub = create_submit_df(df_preds_auctions.set_index('ref_hash'), \\\n",
    "                              df_preds_installs.set_index('ref_hash'), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(kaggle_sub, \"submit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_sub_neural = create_submit_df(df_preds_auctions_neural.set_index('ref_hash'), \\\n",
    "                              df_preds_installs_neural.set_index('ref_hash'), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(kaggle_sub_neural, \"submit_neural.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble_df = kaggle_sub.merge(kaggle_sub_neural, on=\"ref_hash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble_df[\"obj\"] = (ensamble_df[\"obj_x\"] + ensamble_df[\"obj_y\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble_kaggle = ensamble_df[[\"ref_hash\", \"obj\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(ensamble_kaggle, \"submit_ensamble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble XGBoost + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_sub_lgbm = create_submit_df(df_preds_auc_gbm.set_index('ref_hash'), df_preds_inst_gbm.set_index('ref_hash'), target)\n",
    "kaggle_sub_xgboost = create_submit_df(df_preds_auctions.set_index('ref_hash'), df_preds_installs.set_index('ref_hash'), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_submit = kaggle_sub_lgbm.merge(kaggle_sub_xgboost, on = 'ref_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_submit['obj'] = (xgboost_submit['obj_x'] + xgboost_submit['obj_y'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_submit = xgboost_submit[['ref_hash', 'obj']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(xgboost_submit, \"submit_leo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble lgbm + random_forests + xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_inst = pd.read_csv(\"installs_predictions_lightgbm.csv\")\n",
    "b_inst = pd.read_csv(\"installs_predictions_randomForest.csv\")\n",
    "c_inst = pd.read_csv(\"installs_predictions_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_auc = pd.read_csv(\"auctions_predictions_lightgbm.csv\")\n",
    "b_auc = pd.read_csv(\"auctions_predictions_randomForest.csv\")\n",
    "c_auc = pd.read_csv(\"auctions_predictions_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_b_inst = a_inst.merge(b_inst, on=\"ref_hash\")\n",
    "all_inst = a_b_inst.merge(c_inst, on = \"ref_hash\")\n",
    "all_inst['obj'] = (all_inst['obj_x'] + all_inst['obj_y'] + all_inst['obj'])/3\n",
    "all_inst = all_inst[['ref_hash', 'obj']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_b_auc = a_auc.merge(b_auc, on=\"ref_hash\")\n",
    "all_auc = a_b_auc.merge(c_auc, on = \"ref_hash\")\n",
    "all_auc['obj'] = (all_auc['obj_x'] + all_auc['obj_y'] + all_auc['obj'])/3\n",
    "all_auc = all_auc[['ref_hash', 'obj']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = create_submit_df(all_auc.set_index('ref_hash'), all_inst.set_index('ref_hash'), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(submit, \"submit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit con LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(create_submit_df(df_preds_auc_gbm.set_index('ref_hash'), df_preds_inst_gbm.set_index('ref_hash'), target), \"submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
